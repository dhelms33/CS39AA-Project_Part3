{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 3\n",
    "\n",
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/dereckhelms/project-part3-dh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creation of Feed-Forward Model\n",
    "Although I do not believe a feed-forward model is the most appropriate for this set of structured data, I want to construct one to see how it behaves in relation to another deep learning model. \n",
    "Below are the imports I will use for this section.\n",
    "It is necessary to convert the binary smoker feature to one hot encoding for the model to analyze it without errors. StandardScaler is used to scale the data to a normal distribution and if all the data is not of float type I will receive many errors.\n",
    "\n",
    "NOTE: Due to the restrictions of my windows machine, I am unable to run the results locally, but everything in Kaggle can be run and analyzed. \n",
    "\n",
    "The model has been fit for the values of smoker where yes and no have been transformed into binary values. As 'features['smoker'] = label_encoder.fit_transform(features['smoker'])' suggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Desktop\\CS\\CS39AA-Project-DH\\project_part3.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Desktop/CS/CS39AA-Project-DH/project_part3.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Desktop/CS/CS39AA-Project-DH/project_part3.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Desktop/CS/CS39AA-Project-DH/project_part3.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Desktop/CS/CS39AA-Project-DH/project_part3.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Desktop/CS/CS39AA-Project-DH/project_part3.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/kaggle/input/healthcare-insurance/insurance.csv')\n",
    "\n",
    "# Extract features and target variable\n",
    "features = df[['age', 'bmi', 'children', 'smoker', 'region']]\n",
    "target = df['charges']\n",
    "\n",
    "# Explicitly create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "features = features.copy()\n",
    "\n",
    "# Convert categorical features to one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "features['smoker'] = label_encoder.fit_transform(features['smoker'])\n",
    "features = pd.get_dummies(features, columns=['region'])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "features[['age', 'bmi', 'children']] = scaler.fit_transform(features[['age', 'bmi', 'children']])\n",
    "\n",
    "# Ensure all data is of numeric type\n",
    "features = features.astype('float32')\n",
    "target = target.astype('float32')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Define the model architecture\n",
    "class InsuranceModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(InsuranceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Adjust the number of hidden units as needed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)  # Output is a single number for insurance charge prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create a model instance\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = InsuranceModel(input_size)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust the learning rate as needed\n",
    "\n",
    "# Lists to store training loss for visualization\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X_train_tensor)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Append the training loss to the list\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Print training information\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Plot the training loss over epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_test_tensor, test_predictions)\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "\n",
    "# Plot predicted versus actual values on the test set\n",
    "plt.scatter(y_test_tensor, test_predictions, alpha=0.5)\n",
    "plt.xlabel('Actual Charges')\n",
    "plt.ylabel('Predicted Charges')\n",
    "plt.title('Actual vs. Predicted Charges on Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print some predicted versus actual values\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    sample_indices = np.random.choice(len(X_test_tensor), 5, replace=False)\n",
    "    for i in sample_indices:\n",
    "        prediction = model(X_test_tensor[i])\n",
    "        print(f'Sample {i + 1}: Predicted={prediction.item():.4f}, Actual={y_test_tensor[i].item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Lists to store training loss for visualization\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "# Plot the training loss over epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Plot predicted versus actual values on the test set\n",
    "plt.scatter(y_test_tensor, test_predictions, alpha=0.5)\n",
    "plt.xlabel('Actual Charges')\n",
    "plt.ylabel('Predicted Charges')\n",
    "plt.title('Actual vs. Predicted Charges on Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print some predicted versus actual values\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    sample_indices = np.random.choice(len(X_test_tensor), 5, replace=False)\n",
    "    for i in sample_indices:\n",
    "        prediction = model(X_test_tensor[i])\n",
    "        print(f'Sample {i + 1}: Predicted={prediction.item():.4f}, Actual={y_test_tensor[i].item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test loss of a nearly constant 314447200.0000 was unexpectedly large. It was assumed that the test loss was going to be significantly higher than in part 2 since deep models do not perform the best on structured data. However, when one observes the MAE of the random forest was = 2755.556764385067 and in comparision the MAE of the feed-forward model was = 12633.0352 which is nearly 6 times larger. In the next section I will see if adjusting the hyperparameters (learning rate and dropout rate) will improve the MAE and test loss.\n",
    "I will also add a validation data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding validation graphs and adjusting hyperparameters of Feed-Forward Model\n",
    "Below I changed the dropout rate to .7, changed the learning rate to .006 to increase the nodes that drop out and to take more granular steps across the model to find new minima. Additionally, I added training and validation graphs to see how the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'features' is your input data and 'target' is the corresponding target variable (charges)\n",
    "# Update this based on your actual dataset\n",
    "# features = ...\n",
    "# target = ...\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Define the model architecture\n",
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.7):\n",
    "        super(FeedForwardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set the input size, hidden layer sizes, and output size\n",
    "input_size = features.shape[1]\n",
    "hidden_size1 = 64  # Adjust as needed\n",
    "hidden_size2 = 32  # Adjust as needed\n",
    "output_size = 1\n",
    "dropout_rate = 0.7  # Adjust as needed\n",
    "\n",
    "# Instantiate the model\n",
    "model = FeedForwardModel(input_size, hidden_size1, hidden_size2, output_size, dropout_rate)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.006)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000  # Adjust as needed\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and training loss\n",
    "    model.train()\n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Save losses for plotting\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Print training information\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    mae_test = mean_absolute_error(y_test_tensor.numpy(), test_predictions.numpy())\n",
    "    print(f'Mean Absolute Error on Test Set: {mae_test:.4f}')\n",
    "\n",
    "# Visualize results\n",
    "plt.scatter(y_test_tensor.numpy(), test_predictions.numpy(), alpha=0.5)\n",
    "plt.xlabel('Actual Charges')\n",
    "plt.ylabel('Predicted Charges')\n",
    "plt.title('Actual vs. Predicted Charges on Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine tuning of hyper parameters drastically decreased the MAE to 8751.6582, only 4 times larger than the MAE of the random forest model. Since fine tuning these hyper parameters drastically decreased the MAE, these parameters and this model will be selected for the final model of part 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Increasing the lr and dropout rate of Feed-Forward Model\n",
    "Below I changed the dropout rate to .8, changed the learning rate to .007 to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'features' is your input data and 'target' is the corresponding target variable (charges)\n",
    "# Update this based on your actual dataset\n",
    "# features = ...\n",
    "# target = ...\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Define the model architecture\n",
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.8):\n",
    "        super(FeedForwardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set the input size, hidden layer sizes, and output size\n",
    "input_size = features.shape[1]\n",
    "hidden_size1 = 64  # Adjust as needed\n",
    "hidden_size2 = 32  # Adjust as needed\n",
    "output_size = 1\n",
    "dropout_rate = 0.8  # Adjust as needed\n",
    "\n",
    "# Instantiate the model\n",
    "model = FeedForwardModel(input_size, hidden_size1, hidden_size2, output_size, dropout_rate)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.007)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000  # Adjust as needed\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and training loss\n",
    "    model.train()\n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Save losses for plotting\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Print training information\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    mae_test = mean_absolute_error(y_test_tensor.numpy(), test_predictions.numpy())\n",
    "    print(f'Mean Absolute Error on Test Set: {mae_test:.4f}')\n",
    "\n",
    "# Visualize results\n",
    "plt.scatter(y_test_tensor.numpy(), test_predictions.numpy(), alpha=0.5)\n",
    "plt.xlabel('Actual Charges')\n",
    "plt.ylabel('Predicted Charges')\n",
    "plt.title('Actual vs. Predicted Charges on Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAE decreased further to 8554.5703, so I will continue to increase the learning rate and dropout rate to see if the MAE continues to decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Increasing the lr and dropout rate of Feed-Forward Model\n",
    "Below I changed the dropout rate to .9, changed the learning rate to .008 to see the further hypothesized improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'features' is your input data and 'target' is the corresponding target variable (charges)\n",
    "# Update this based on your actual dataset\n",
    "# features = ...\n",
    "# target = ...\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Define the model architecture\n",
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.9):\n",
    "        super(FeedForwardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set the input size, hidden layer sizes, and output size\n",
    "input_size = features.shape[1]\n",
    "hidden_size1 = 64  # Adjust as needed\n",
    "hidden_size2 = 32  # Adjust as needed\n",
    "output_size = 1\n",
    "dropout_rate = 0.9  # Adjust as needed\n",
    "\n",
    "# Instantiate the model\n",
    "model = FeedForwardModel(input_size, hidden_size1, hidden_size2, output_size, dropout_rate)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.008)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000  # Adjust as needed\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and training loss\n",
    "    model.train()\n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(X_val_tensor)\n",
    "        val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "    # Save losses for plotting\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # Print training information\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    mae_test = mean_absolute_error(y_test_tensor.numpy(), test_predictions.numpy())\n",
    "    print(f'Mean Absolute Error on Test Set: {mae_test:.4f}')\n",
    "\n",
    "# Visualize results\n",
    "plt.scatter(y_test_tensor.numpy(), test_predictions.numpy(), alpha=0.5)\n",
    "plt.xlabel('Actual Charges')\n",
    "plt.ylabel('Predicted Charges')\n",
    "plt.title('Actual vs. Predicted Charges on Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAE continued to decrease to 8526.3916; however the validation error in the set continue to increase. This showed that there was a point of diminishing returns and that the model was overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graphs for Comparison\n",
    "This is the same residual graph I used in part 2 and will use this to measure the success of the feed forward model. The residual graph shows that the feed forward model is not as accurate as the random forest model. The feed forward model has a much larger range of residuals and the residuals are not as close to 0 as the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_test_tensor, test_predictions)\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
